{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_preliminaries_lookup-api.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoWIN963cIdvLFTzLicPih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljm1128/d2l/blob/main/chapter_preliminaries_lookup_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG9FXzGv6Jks",
        "outputId": "aba00c75-2ca3-49ff-f535-3c96351c159a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(dir(torch.distributions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(torch.tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAp81Qxi6lJ0",
        "outputId": "c96d8e53-ae31-48de-e8fa-0a4c9860f919"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function tensor:\n",
            "\n",
            "tensor(...)\n",
            "    tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
            "    \n",
            "    Constructs a tensor with :attr:`data`.\n",
            "    \n",
            "    .. warning::\n",
            "    \n",
            "        :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor\n",
            "        ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
            "        or :func:`torch.Tensor.detach`.\n",
            "        If you have a NumPy ``ndarray`` and want to avoid a copy, use\n",
            "        :func:`torch.as_tensor`.\n",
            "    \n",
            "    .. warning::\n",
            "    \n",
            "        When data is a tensor `x`, :func:`torch.tensor` reads out 'the data' from whatever it is passed,\n",
            "        and constructs a leaf variable. Therefore ``torch.tensor(x)`` is equivalent to ``x.clone().detach()``\n",
            "        and ``torch.tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``.\n",
            "        The equivalents using ``clone()`` and ``detach()`` are recommended.\n",
            "    \n",
            "    Args:\n",
            "        data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
            "            NumPy ``ndarray``, scalar, and other types.\n",
            "    \n",
            "    Keyword args:\n",
            "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "            Default: if ``None``, infers data type from :attr:`data`.\n",
            "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            "            Default: if ``None``, uses the current device for the default tensor type\n",
            "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
            "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
            "        requires_grad (bool, optional): If autograd should record operations on the\n",
            "            returned tensor. Default: ``False``.\n",
            "        pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
            "            the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
            "    \n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
            "        tensor([[ 0.1000,  1.2000],\n",
            "                [ 2.2000,  3.1000],\n",
            "                [ 4.9000,  5.2000]])\n",
            "    \n",
            "        >>> torch.tensor([0, 1])  # Type inference on data\n",
            "        tensor([ 0,  1])\n",
            "    \n",
            "        >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
            "        ...              dtype=torch.float64,\n",
            "        ...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\n",
            "        tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
            "    \n",
            "        >>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\n",
            "        tensor(3.1416)\n",
            "    \n",
            "        >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
            "        tensor([])\n",
            "\n"
          ]
        }
      ]
    }
  ]
}